{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8498dd77-e174-4c10-a2b1-fd4e6788d489",
   "metadata": {},
   "source": [
    "# 40-audio-inference\n",
    "> Starting to use audio for inference\n",
    "\n",
    "In this notebook, we start to investigate using wav2vec2 and the associated classes on HuggingFace.  We'll start by trying straight transcription using the classes themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b632d1-017f-4388-bbe9-981f6cdf1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_no_test\n",
    "#modeling imports\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "#python imports\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feecf0e-2998-4dea-a7c3-02c936d7616f",
   "metadata": {},
   "source": [
    "# Models\n",
    "Here, we'll use the facebook wav2vec2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd71a2-6d30-4f16-ade1-178899606224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "pretrain_name1 = \"facebook/wav2vec2-base-960h\"\n",
    "pretrain_name2 = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "\n",
    "# create processors and models\n",
    "processor = Wav2Vec2Processor.from_pretrained(pretrain_name1)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(pretrain_name1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfbb23a-ed0c-4ace-aea8-c89e5067a424",
   "metadata": {},
   "source": [
    "# Sample demo\n",
    "In this example, clean audio is directly recorded and attempted for transcription.  The audio is found on Box at the location indicated below.  Given the text about wav2vec2 below on HuggingFace:\n",
    ">The base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n",
    "\n",
    "We make sure that the text has the correct sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7115151-d460-42b3-b257-e5bfa6e03f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample rate of test audio:  16000\n",
      "The length of audio numpy: (60929,)\n"
     ]
    }
   ],
   "source": [
    "# Use test file on Box and read it\n",
    "myd, mys = sf.read(os.path.expanduser('~/Box/DSI Documents/test_files/mytest.wav')) # my own short test audio\n",
    "print(\"The sample rate of test audio: \", mys)\n",
    "print(\"The length of audio numpy:\", myd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae5f2e-9e26-4a8d-b5b4-cf83bc83bd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WHO ARE YOU MY NAME IS MICHAEL']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "input_values = processor(myd, return_tensors = \"pt\", sampling_rate = 16000).input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3eccb-10a5-4a57-bbbe-4481b8f10eb6",
   "metadata": {},
   "source": [
    "As we can see that the first word \"How\" is interpreted as \"Who\". It's pretty good when the audio is clear and simple sentence. Let's see what gonna happen when transcribing class audio.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e44a8-6502-4b4f-a7ed-7fd06e8e9bb6",
   "metadata": {},
   "source": [
    "# Class audio\n",
    "Now, we'll use the transformer model to embed audio numpy and then decode it as words.  I only chose first 2 million audio numpy to train wav2vec because using all 13 million would crash our memory no matter how big your memory is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194aa22-c1db-425e-badb-9af22c16838f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample rate of class audio:  16000\n",
      "The length of audio array: (13089280,)\n"
     ]
    }
   ],
   "source": [
    "# Use test file on Box and read it\n",
    "class_audio, class_sr = sf.read(os.path.expanduser('~/Box/DSI Documents/cleaned_data/resampled_audio_16khz/008-1.wav')) # my own short test audio\n",
    "print(\"The sample rate of class audio: \", class_sr)\n",
    "print(\"The length of audio array:\", class_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7e009-5b88-4d38-8591-3c568d7ef2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MONIDEGO ACORN AND YO IT ERY NY GO NOT TE BE ANY WAY HAPING ACON THE WOW NOT OT TE TAT TE GERY TE LAT  G A CORN AR HEL AD WAT WA GROD TO BE GOL GAT THE WA TAT GRD O GAD GOL WO A GA T  GO WAY OR REE AN THE RAT OR TE GREE  GRL O IN THE WO OA ON TE NA THE GAT TE RD  GRA O THE TA TE TA ROL W HER O GOW VERY  E  E EN TH GO A TA A GRAT MA GOWL AN E KNOW THAT YOU NO WA OR TE GRA TE GROL E GOD TE TAT TA E E TAT AAD TOT EVERYTHING A GRO GO MELY GO WA A BE IF ATE TO BE A A GO WIT TE A NAT AND O O BE A TA IN AGRAT TOD OR NOT AT YET A TAT AND A A TE AN HE TA A GOL NO OAD AN A NEE GA I HAD TA Y NOT TE  NOT YE NO BO  BE TE O GRAD NA HAN O RA  AT W AD AT WI TE GE R O RAT AN NO THE BERY OL TE AT  NEW TAT TAT AD T  A GO A A AN RAN  MAD TE E GIV ON  BAY IF TE TE MAD A GON E GR GOON  E    O GD NA TAT TE THE MAN TA GIV ON BE A TAT']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize first 2 million numpy data\n",
    "input_values = processor(class_audio[:2000000], return_tensors = \"pt\", sampling_rate = 16000).input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d4091-f3c0-44d7-837c-b413a7becd93",
   "metadata": {},
   "source": [
    "Note that the transcription is not good in any sense.  We shouldn't particularly be overly worried about this, as there are multiple very challenging steps that are required for going from audio to the analogous words.  Keep in mind that the audio representation to text is difficult enough; we require the efforts up to audio representation, and the transcription component itself is relatively superfluous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebb558-3089-41f7-be5d-8fdd2be693e9",
   "metadata": {},
   "source": [
    "# A brief aside on loading in data\n",
    "As is clear, we're using soundfile to load in the audio, which is sampled at 16000Hz (16000 samples per second).  We can use some of the functionality of this package to specifically state which subsets of the audio we want to load in.  Alternately, we can use the loaded in file (currently `class_audio`) and get the segments that we want.  Let's check out what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541e8d9-42dd-41c3-862b-dbbe927029a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "sampling_rate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a1b09-2998-4698-b390-aefcbc90b948",
   "metadata": {},
   "source": [
    "Let's say we want to start at 10 seconds in and have a duration of 4 seconds.  Let's see what the indices of these would be in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b8c36-ec10-46c3-8295-a925fec3c13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at: 160000 and end at: 224000\n"
     ]
    }
   ],
   "source": [
    "#do some math to get start and end indices (in the signal array)\n",
    "start_index = 10*sampling_rate\n",
    "end_index = start_index + 4*sampling_rate\n",
    "print('Start at:', start_index, 'and end at:', end_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357048d1-a9a3-4883-95c0-7d8bcc80c555",
   "metadata": {},
   "source": [
    "Let's see how we can use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d897ca-297c-4c28-8918-9f2572f33727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACORN AND YOU ARE WITH A VERY NIFE']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process subset of audio\n",
    "input_values = processor(class_audio[start_index:end_index+1], return_tensors = \"pt\", sampling_rate = sampling_rate).input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74586b60-dac0-4fd3-a713-21f271808a50",
   "metadata": {},
   "source": [
    "This transcription appears to be substantially better since it is at a better spot in the data.  Also, shorter sequences appear to work better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
