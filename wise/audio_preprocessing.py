# AUTOGENERATED! DO NOT EDIT! File to edit: 42-wav-embedding-preprocess.ipynb (unless otherwise specified).

__all__ = ['to_millseconds', 'short_warning', 'preprocess_audio_segments_csv', 'add_audio_embeddings_info',
           'check_label', 'write_nd_parquet', 'get_fixed_length_segments', 'short_embedding_csv_load']

# Cell
# modeling packages
from transformers import Wav2Vec2Processor, Wav2Vec2Model
import soundfile as sf
import torch
import librosa
import warnings
import difflib

# data science packages
import pandas as pd
import numpy as np

#saving data format files
import pyarrow as pa
import pyarrow.parquet as pq

# other python packages
import os.path
import glob
import re

# Cell
def to_millseconds(time):
    '''
    Function to_millseconds: converts time with timestamp string of format '\d\d:\d\d.\d*' to milliseconds
    Inputs: time: String in required format
    Outputs: integer of converted time in milliseconds
    '''

    if isinstance(time, str)==False:
        raise TypeError('The input datatype of {0} must be a string to use to_milliseconds.'.format(time))

    #Timestamp pattern to use later
    ts_target = re.compile('\d{2}:\d{2}\.\d{1,}')
    if ts_target.match(time) is None:
        raise RuntimeError("The input of {0} does not match the format of \d\d:\d\d.\d*. Fix this before continuing.")

    #get split pieces
    sp = re.split(":|\.", time)

    #get milliseconds
    ms = int(sp[0])*60*1000
    ms = ms + float(sp[1] + '.' + sp[2])*1000
    ms = int(ms)

    return ms

# Cell
def short_warning(message):
    '''
    Function short_warning: shortened version of warnings.warn_explicit to remove unnecessary echo
        Input: message to be printed as warning message
        Output: warning
    '''
    warnings.warn_explicit(message, UserWarning, '', 0)

# Internal Cell
def _fix_added_timestamp(row_info):
    '''
    Function _fix_added_timestamp: validates timestamps and tries to fix them; returns a df with column
    `fatal_error` included. This is a pandas helper function and should not be applied directly without .apply.
    Input: row_info: pandas Series corresponding to a single row
    Returns: row_info with corrected timestamps or same timestamp with a new column 'fatal_error' with 1 if the
    timestamp could not be successfully converted.
    '''

    #Timestamp pattern to use later
    ts_target = re.compile('\d{2}:\d{2}\.\d{3}')

    #Keep count of fatal errors
    fatal_errors = 0

    for ts_type in ['start_timestamp', 'end_timestamp']:

        #Make sure it's a string
        if isinstance(row_info[ts_type], str)==False:
            short_warning('{0}: Row {1} has a {2} that is not a string with value {3}. Cannot automatically fix.'
                          .format(row_info['id'], row_info.name, ts_type, row_info[ts_type]))
            fatal_errors = fatal_errors + 1
            continue

        #See if it has too many segments
        ts_pieces = re.split(":|\.", row_info[ts_type])
        if len(ts_pieces) != 3:
            if len(ts_pieces) == 4:
                short_warning('{0}: Row {1} {2} with value {3} has 4 time parts instead of 3. Automatically fixing...'
                              .format(row_info['id'], row_info.name, ts_type, row_info[ts_type]))

                ts_pieces = ts_pieces[1:4]
                row_info[ts_type] = ts_pieces[0] + ':' + ts_pieces[1] + '.' + ts_pieces[2]
            else:
                short_warning('{0}: Row {1} with value {2} has {3} pieces in {4} and cannot be fixed automatically. Please amend.'
                             .format(row_info['id'], row_info.name, row_info[ts_type], len(ts_pieces), ts_type))
                fatal_errors = fatal_errors + 1
                continue

        #If it's perfect, let's just be done
        if ts_target.match(row_info[ts_type]) is not None:
            continue

        #Otherwise, let's get it into the right format
        ts_pieces[0] = ts_pieces[0].rjust(2,'0')
        ts_pieces[1] = ts_pieces[1].rjust(2,'0')
        ts_pieces[2] = ts_pieces[2].ljust(3,'0')

        #Update values
        short_warning('{0}: Row {1} {2} has the incorrect format of {3}. Automatically fixing...'
                      .format(row_info['id'], row_info.name, ts_type, row_info[ts_type]))
        row_info[ts_type] = ts_pieces[0] + ':' + ts_pieces[1] + '.' + ts_pieces[2]

    #Save fatal errors
    row_info['fatal_errors'] = fatal_errors

    return row_info


# Cell
def preprocess_audio_segments_csv(csv_df, duration_max=15000):
    '''
    Function preproces_audio_segments_csv: pre-processes manually-entered timestamps to ensure correct format
    Inputs: csv_df: original dataframe with at least columns start_timestamp, end_timestamp, and id
            duration_max (default 15000): maximum length allowed for an utterance
    Returns: pandas dataframe with corrected or dropped timestamps, corresponding timestamps in ms, and duration
    '''

    #Drop unwanted "Unnamed" columns
    drop_cols = [drop_col for drop_col in csv_df.columns if drop_col.startswith('Unnamed')]
    csv_df.drop(columns=drop_cols, inplace=True)

    #Strip any leading or trailing whitespace
    csv_df['start_timestamp'] = csv_df['start_timestamp'].str.strip()
    csv_df['end_timestamp'] = csv_df['end_timestamp'].str.strip()

    #See if we need to drop NAs and notify of drops
    na_sz = len(csv_df.dropna(subset=['start_timestamp', 'end_timestamp']))
    if na_sz != len(csv_df):
        orig_sz = len(csv_df)
        csv_df.dropna(subset=['start_timestamp', 'end_timestamp'], inplace=True)
        short_warning("You had {0} NA rows in start_timestamp or end timestamp which were dropped."
                      .format(na_sz))

    #See if we have wrong formats on timestamps and process or notify
    csv_df = csv_df.apply(_fix_added_timestamp, axis='columns')

    #Determine if the df can continue forward based on timestamps
    no_fatal_errors = csv_df['fatal_errors'].sum()
    if no_fatal_errors != 0:

        #display errors and get all rows except those with fatal errors
        error_rows = csv_df.query('fatal_errors!=0')
        short_warning('File {0} has {1} timestamp errors that cannot be automatically corrected. Dropping these rows.\nDropped row summary due to timestamp (truncated table):\n{2}'
                      .format(csv_df['id'][0], no_fatal_errors, error_rows[['id', 'start_timestamp', 'end_timestamp']]))
        csv_df = csv_df.drop(index=error_rows.index)

    #Convert times to milliseconds and calculate duration
    csv_df["start_ms"] = csv_df["start_timestamp"].apply(to_millseconds)
    csv_df["end_ms"] = csv_df["end_timestamp"].apply(to_millseconds)
    csv_df["duration_ms"] = csv_df['end_ms'] - csv_df["start_ms"]

    #Validate ms
    csv_df['fatal_errors'] = csv_df['duration_ms'].apply(lambda x: 0 if x > 0 else 1)
    csv_df['fatal_errors'] = csv_df.apply(lambda x: x['fatal_errors'] if x['duration_ms'] <= duration_max else 1,
                                         axis=1)
    no_fatal_errors = csv_df['fatal_errors'].sum()
    if no_fatal_errors != 0:

        #display errors and get all rows except those with fatal errors
        error_rows = csv_df.query('fatal_errors!=0')
        short_warning('File {0} has {1} time duration issues. Dropping these rows.\nDropped row summary due to duration (truncated table):\n{2}'
                      .format(csv_df['id'][0], no_fatal_errors,
                              error_rows[['id', 'start_ms', 'end_ms', 'duration_ms']]))
        csv_df = csv_df.drop(index=error_rows.index)

    #Once we've removed fatal errors (or have no fatal errors, drop the column and return)
    csv_df.drop(columns=['fatal_errors'], inplace=True)

    #Get the indices together correctly
    csv_df.reset_index(drop=True, inplace=True)

    return csv_df

# Internal Cell
def _get_audio_embeddings(row_info, wav_file, aud_processor, aud_mdl, samp_rate):
    '''
    Function _get_audio_embeddings: generates embeddings for a wave file using a model. Function not to be used
    directly without pandas .apply function.
    Inputs: row_info: pandas Series of row info with minimally start_index and end_index
            wav_file: list or numpy array of wave file
            aud_processor: huggingface audio processor for inputs
            aud_mdl: huggingface audio model to generate embeddings
            samp_rate: sampling rate of audio
    Outputs: pandas Series of row info with added columns 'last_hidden_state',
    'shape_state', and 'last_hidden_state_mean'
    '''

    #Get the processed input values using the processor
    input_values = aud_processor(wav_file[row_info['start_index'] : row_info['end_index']],
                                 return_tensors="pt", sampling_rate = samp_rate).input_values

    #Get the embeddings values
    last_hidden_state = aud_mdl(input_values).last_hidden_state[0,:,:]
    row_info['last_hidden_state'] = last_hidden_state.tolist()
    row_info['shape_state'] = list(last_hidden_state.shape)
    row_info['last_hidden_state_mean'] = torch.mean(last_hidden_state, dim=0).tolist()

    #Return
    return row_info

# Cell
def add_audio_embeddings_info(pd_audio,
                              audio_no,
                              audio_processor,
                              audio_mdl,
                              sampling_rate = 16000,
                              base_prefix = "/data/p_dsi/wise/data/resampled_audio_16khz/"):
    '''
    Input argument:
        pd_audio: cleaned dataframe with cleaned start and end timestamps (correctly formatted into xx:xx.xxx)
        audio_no: String of audio_number (e.g., '083-1')
        audio_processor: HF audio processor (e.g., instantiated Wav2Vec2Processor)
        audio_mdl: HF audio base model (e.g., instantiated Wav2Vec2Model)
        sampling_rate (default 16000): integer of sampling rate of audio
        base_prefix (default '/data/p_dsi/wise/data/resampled_audio_16khz'): String of filepath to audio files
    Output:
        a pandas dataframe containing original csv file and addition columns including last hidden states matrix and vector
    '''

    #Print some info
    print('Working on file:', audio_no)

    #Read in timestamp csv file and corresponding audio file
    audio_wave, sr = sf.read(base_prefix + audio_no + '.wav')

    #Calculate indices in audio file
    cal_index = lambda x: int(x) * (sampling_rate // 1000)
    pd_audio["start_index"] = pd_audio["start_ms"].apply(cal_index)
    pd_audio["end_index"] = pd_audio["end_ms"].apply(cal_index)

    #Add embeddings information
    pd_audio = pd_audio.apply(lambda x: _get_audio_embeddings(x, audio_wave,
                                                              audio_processor, audio_mdl, sampling_rate),
                             axis='columns')

    #Reset index to make sure continuous numbering
    pd_audio.reset_index(drop=True, inplace=True)

    #Return
    return pd_audio

# Internal Cell
def _check_label(row, label_list):
    '''
    Function _check_label: Internal helper function with .apply in pandas to check label. Not to be used directly.
    Inputs: row: pandas Series of dataframe row with minimially 'label' column
            label_list: list of accepted labels in df
    Returns: warning or fixed label in row
    '''

    if row['label'] not in label_list:
        #Get match ratio
        matches = [difflib.SequenceMatcher(a=row['label'].lower(), b=test_label.lower()).ratio()
                   for test_label in label_list]

        #Get index of best match and set it
        maxindex = np.argmax(matches)
        best_label = label_list[maxindex]

        short_warning('File {0}: Row {1} has label {2}; replaced with {3}'
                      .format(row['id'], row.name, row['label'], best_label))

        #Fix
        row['label'] = best_label

    return row

# Cell
def check_label(df, label_list=None):
    """
    Check if there is any wrong labels in df
    Inputs: df: pandas data frame
            label_list (default None): list of accepted label names in label column or None to use defaults
    Output: throw warnings when encountering wrong labels, returns corrected labels
    """

    if label_list is None:
        label_list = ["OTR", "NEU", "REP", "PRS"]

    #Make sure label is right
    df = df.apply(lambda x: _check_label(x, label_list), axis='columns')

    return df

# Cell
def write_nd_parquet(df, filepath):
    '''
    Function write_nd_parquet: writes a parquet file with complex columns. May be unnecessary.
    Inputs: df: dataframe to be written
            filepath: full filepath for output
    Output: None, prints the filepath that the dataframe was written to.
    '''

    #Convert to table
    pq_table = pa.Table.from_pandas(df)

    #Save file
    pq.write_table(pq_table, filepath)
    print('Wrote dataframe to:', filepath)

    return

# Comes from 45-restructure-audio-fixed-length.ipynb, cell
# data science packages
import pandas as pd
import numpy as np

# other python packages
import os.path
import glob
import re

# Comes from 45-restructure-audio-fixed-length.ipynb, cell
def _group_statements(group_info):
    '''
    Function _group_statements: pandas apply helper function to group sets of statements into one fixed length row
    Input: group_info: pandas group with minimally elements id, speech, label, label_id, start_ms, end_ms,
                       start_timestamp, end_timestamp, duration_ms
    Output: new dataframe with single row for sets of statements
    '''

    #Get overall info
    row_id = group_info['id'].iloc[0]
    speech_list = group_info['speech'].tolist()
    speech = ' '.join(speech_list)
    label = group_info['label'].tolist()
    label_id = group_info['label_id'].tolist()

    #Get start info
    start_ms = group_info['start_ms'].iloc[0]
    start_timestamp = group_info['start_timestamp'].iloc[0]
    start_index = group_info['start_index'].iloc[0]

    #Get end info
    end_ms = group_info['end_ms'].iloc[-1]
    end_timestamp = group_info['end_timestamp'].iloc[-1]
    end_index = group_info['end_index'].iloc[-1]

    #Get duration info
    duration_ms = group_info['duration_ms'].sum()

    #Make dataframe
    df = pd.DataFrame({'id':row_id, 'speech_list':[speech_list], 'speech':speech,
                       'label':[label], 'label_id':[label_id],
                       'start_timestamp':start_timestamp, 'end_timestamp':end_timestamp,
                       'start_ms':start_ms, 'end_ms':end_ms, 'duration_ms':duration_ms,
                       'start_index': start_index, 'end_index':end_index},
                     index=['1'])

    return df

# Comes from 45-restructure-audio-fixed-length.ipynb, cell
def _add_label_counts(row_info):
    '''
    Function _add_label_counts: helper function for pandas apply; adds label counts as individual columns. Not to
    be used directly.
    Inputs: row_info: pandas Series of row info with minimally 'label'
    Output: returns pandas Series of row info with new label counts added for that row.
    '''

    #Get counts of labels
    vc = pd.Series(row_info['label']).value_counts()

    #Add it back info the index
    row_info[vc.index]=vc

    return row_info


# Comes from 45-restructure-audio-fixed-length.ipynb, cell
def get_fixed_length_segments(csv_df, length_in_ms=2000, label_list=None):
    '''
    Function get_fixed_length_segments: Function to regroup dataframe into fixed length segments
    Inputs: csv_df: dataframe with minimally speech, label, all timestamps, all milliseconds, duration, and indices.
            length_in_ms (default 2000): integer of time of fixed length in milliseconds
            label_list (default None): list of accepted labels in dataframe; default label list used if None
    Outputs: regrouped dataframe with one row per fixed length statements lengths with counts of each label
    '''

    #Make label list and generate encodings in df
    if label_list is None:
        label_list = ["OTR", "NEU", "REP", "PRS"]

    #Create label encoding
    label2id = {lab:ind for ind, lab in enumerate(label_list)}

    #Do the encoding
    csv_df['label_id'] = csv_df['label'].replace(label2id)

    #Add groups
    csv_df['ts_group'] = csv_df['end_ms']//length_in_ms

    #Get groups and get in a reasonable format
    csv_df = csv_df.groupby('ts_group').apply(_group_statements).reset_index(drop=True)

    #Add an area for the label counts to be filled in
    csv_df[[label_list]]=0

    #Add label counts
    csv_df = csv_df.apply(_add_label_counts, axis=1)

    #All done!
    return csv_df


# Comes from 45-restructure-audio-fixed-length.ipynb, cell
def short_embedding_csv_load(fname):
    '''
    Function short_embedding_csv_load: Function to load a subset of data from input parquet file
    Input: String of full filepath
    Output: dataframe with only columns of interest
    '''

    df = pd.read_parquet(fname,
                         columns=['id', 'speech', 'label', 'start_timestamp', 'end_timestamp',
                                  'start_ms', 'end_ms', 'duration_ms',
                                  'start_index', 'end_index'])

    return df