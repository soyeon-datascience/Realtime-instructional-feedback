{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12-training-dev-test\n",
    "> Creating functions to split the data into training, validation, and test sets\n",
    "\n",
    "The purpose of this notebook is to create a function which will reproducibly add a `split` column onto the training dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful packages and preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_no_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export text_preprocessing\n",
    "#data access and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#splitting the data\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "#python and file system operations\n",
    "import glob\n",
    "import os.path\n",
    "import docx\n",
    "import re\n",
    "\n",
    "# Suppress all warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_prefix = os.path.expanduser('~/Box Sync/DSI Documents/')\n",
    "base_prefix = os.path.expanduser('/data/p_dsi/wise/data/')\n",
    "\n",
    "file_directory = base_prefix + 'Audio Files & Tanscripts/Transcripts'\n",
    "cleaned_transcripts_dir = base_prefix + 'cleaned_data/cleaned_transcripts' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training, validation, and test set based on whole csv output\n",
    "We'll use the `final_csv.csv` file created in 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the final concatenated data and show the head of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_filename = base_prefix + 'cleaned_data/final_csv.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14008, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcript_filepath</th>\n",
       "      <th>wave_filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>end_timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>transcriber_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255-3</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/255...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>(okay) before we pass out our character plain ...</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:02:05.28</td>\n",
       "      <td>OTR</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255-3</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/255...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>this time you're gonna look for four types of ...</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:02:05.28</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255-3</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/255...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>okay.</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:02:05.28</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>255-3</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/255...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>yeah.</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:02:05.28</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255-3</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/255...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>you're gonna work as a whole group actually.</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:02:05.28</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                transcript_filepath  \\\n",
       "0  255-3  ~ise/data/cleaned_data/cleaned_transcripts/255...   \n",
       "1  255-3  ~ise/data/cleaned_data/cleaned_transcripts/255...   \n",
       "2  255-3  ~ise/data/cleaned_data/cleaned_transcripts/255...   \n",
       "3  255-3  ~ise/data/cleaned_data/cleaned_transcripts/255...   \n",
       "4  255-3  ~ise/data/cleaned_data/cleaned_transcripts/255...   \n",
       "\n",
       "                                       wave_filename  \\\n",
       "0  /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "1  /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "2  /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "3  /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "4  /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "\n",
       "                                              speech start_timestamp  \\\n",
       "0  (okay) before we pass out our character plain ...     00:00:00.00   \n",
       "1  this time you're gonna look for four types of ...     00:00:00.00   \n",
       "2                                              okay.     00:00:00.00   \n",
       "3                                              yeah.     00:00:00.00   \n",
       "4       you're gonna work as a whole group actually.     00:00:00.00   \n",
       "\n",
       "  end_timestamp label  transcriber_id  \n",
       "0   00:02:05.28   OTR             198  \n",
       "1   00:02:05.28   NEU             198  \n",
       "2   00:02:05.28   NEU             198  \n",
       "3   00:02:05.28   NEU             198  \n",
       "4   00:02:05.28   NEU             198  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read full csv\n",
    "full_df = pd.read_csv(cleaned_data_filename)\n",
    "\n",
    "print(full_df.shape)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We divide our dataset into three parts by two steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step is to divide the whole dataset into two parts, train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get 10% test set now and 90% training set\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.1, random_state= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop NA in order to use ShuffleSplit function and create a copy of full data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of NA\n",
    "full_df[\"label\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_df[full_df['label'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the frequency of class to avoid only one member in one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['NEU', 'NUE', 'OTR', 'PRS', 'REP'], dtype=object),\n",
       " array([8089,    1, 3690, 1715,  513]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['label'].to_numpy(), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that there are some outputs that are mislabeled.  Let's fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'][df['label'] == \"NO\"] = \"NEU\"\n",
    "df['label'][df['label'] == \"NUE\"] = \"NEU\"\n",
    "df['label'][df['label'] == \"OT\"] = \"OTR\"\n",
    "df['label'][df['label'] == \"OTS\"] = \"OTR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check again the frequency to make sure there are at least 2 members in one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['NEU', 'OTR', 'PRS', 'REP'], dtype=object),\n",
       " array([8090, 3690, 1715,  513]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['label'].to_numpy(), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do one split and set a random_state = 0 to make it reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 1347 12542  5536 ...  5100  2879  9814] TEST: [ 8603  3719  5035 ...  9478 10760 13936]\n",
      "Train size is: 12607\n",
      "Test size is: 1401\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in sss.split(np.zeros(df.shape[0]), df['label'].to_numpy()):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"Train size is:\", len(train_index))\n",
    "    print(\"Test size is:\", len(test_index))\n",
    "    df_train = df.iloc[train_index]\n",
    "    df_test = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12607, 8) (1401, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step is to divide df_train got previously into final training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We divide by 1/9 percentage of current new training set\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 1/9, random_state= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 8804  2368 12230 ...  9144  3764 10815] Validation: [10843  3496  1403 ...  5609  5333   175]\n",
      "Train size is: 11206\n",
      "Validation size is: 1401\n"
     ]
    }
   ],
   "source": [
    "for train_index, dev_index in sss.split(np.zeros(df_train.shape[0]), df_train['label'].to_numpy()):\n",
    "    print(\"TRAIN:\", train_index, \"Validation:\", dev_index)\n",
    "    print(\"Train size is:\", len(train_index))\n",
    "    print(\"Validation size is:\", len(dev_index))\n",
    "    df_train = df.iloc[train_index]\n",
    "    df_dev = df.iloc[dev_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we got all three parts: df_train, df_dev, df_test\n",
    "### Add a split column to indicate train = 0, validation = 1, test = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"split\"] = 0\n",
    "df_dev[\"split\"] = 1\n",
    "df_test[\"split\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcript_filepath</th>\n",
       "      <th>wave_filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>end_timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>transcriber_id</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8804</th>\n",
       "      <td>123-1</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/123...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>tuesday.</td>\n",
       "      <td>00:06:27.05</td>\n",
       "      <td>00:08:17.18</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>083-2</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/083...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>you worked real hard to help name get it toget...</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:01:15.06</td>\n",
       "      <td>PRS</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12230</th>\n",
       "      <td>251-3</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/251...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>anything el*&gt;</td>\n",
       "      <td>00:08:05.04</td>\n",
       "      <td>00:09:59.27</td>\n",
       "      <td>OTR</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11125</th>\n",
       "      <td>027-2</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/027...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>ERE.</td>\n",
       "      <td>00:01:16.08</td>\n",
       "      <td>00:02:05.11</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10454</th>\n",
       "      <td>131-1</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/131...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>name where's a space between the words in our ...</td>\n",
       "      <td>00:07:22.23</td>\n",
       "      <td>00:08:39.09</td>\n",
       "      <td>OTR</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                transcript_filepath  \\\n",
       "8804   123-1  ~ise/data/cleaned_data/cleaned_transcripts/123...   \n",
       "2368   083-2  ~ise/data/cleaned_data/cleaned_transcripts/083...   \n",
       "12230  251-3  ~ise/data/cleaned_data/cleaned_transcripts/251...   \n",
       "11125  027-2  ~ise/data/cleaned_data/cleaned_transcripts/027...   \n",
       "10454  131-1  ~ise/data/cleaned_data/cleaned_transcripts/131...   \n",
       "\n",
       "                                           wave_filename  \\\n",
       "8804   /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "2368   /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "12230  /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "11125  /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "10454  /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "\n",
       "                                                  speech start_timestamp  \\\n",
       "8804                                            tuesday.     00:06:27.05   \n",
       "2368   you worked real hard to help name get it toget...     00:00:00.00   \n",
       "12230                                      anything el*>     00:08:05.04   \n",
       "11125                                               ERE.     00:01:16.08   \n",
       "10454  name where's a space between the words in our ...     00:07:22.23   \n",
       "\n",
       "      end_timestamp label  transcriber_id  split  \n",
       "8804    00:08:17.18   NEU             198      0  \n",
       "2368    00:01:15.06   PRS             198      0  \n",
       "12230   00:09:59.27   OTR             198      0  \n",
       "11125   00:02:05.11   NEU             198      0  \n",
       "10454   00:08:39.09   OTR             198      0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "df_train_dev_test = pd.concat([df_train, df_dev, df_test])\n",
    "df_train_dev_test.shape\n",
    "df_train_dev_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to warp all this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export text_preprocessing\n",
    "def add_splits(full_df, seed = 0, test_ratio = 0.1, dev_ratio = 0.1):\n",
    "    '''\n",
    "    This function takes in a Pandas DataFrame and returns a new pandas data frame which uses StratifiedShuffleSplit functions to split them\n",
    "    in accordance with the same distribution of the complete set\n",
    "    train split = 0\n",
    "    validation split = 1\n",
    "    test split = 2\n",
    "    \n",
    "    Argument:\n",
    "    data_name: data file name\n",
    "    seed: control reproducible splits\n",
    "    test_ratio: percentage of test set of the original complete data\n",
    "    dev_ratio: percentage of validation set of the original complete data\n",
    "    '''\n",
    "    \n",
    "    # Drop NA to make sure StratifiedSplit function can run it\n",
    "    df = full_df[full_df['label'].notna()]\n",
    "    \n",
    "    # Correct typo labels\n",
    "    df['label'][df['label'] == \"NO\"] = \"NEU\"\n",
    "    df['label'][df['label'] == \"NUE\"] = \"NEU\"\n",
    "    df['label'][df['label'] == \"OT\"] = \"OTR\"\n",
    "    df['label'][df['label'] == \"OTS\"] = \"OTR\"\n",
    "    \n",
    "    # We get 10% test set now and 90% training set\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = test_ratio, random_state= seed)\n",
    "    \n",
    "    # Split them into training and test set\n",
    "    for train_index, test_index in sss.split(np.zeros(df.shape[0]), df['label'].to_numpy()):\n",
    "        df_train = df.iloc[train_index]\n",
    "        df_test = df.iloc[test_index]\n",
    "        \n",
    "    # We divide by 1/9 percentage of current new training set to get 10% validation set of the original whole dataset\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = dev_ratio / (1 - test_ratio), random_state= seed)\n",
    "    \n",
    "    # Split them into final traning and validation set\n",
    "    for train_index, dev_index in sss.split(np.zeros(df_train.shape[0]), df_train['label'].to_numpy()):\n",
    "        df_train = df.iloc[train_index]\n",
    "        df_dev = df.iloc[dev_index]\n",
    "    \n",
    "    # Add a split column, use train = 0, validation = 1, test = 2\n",
    "    df_train[\"split\"] = 0\n",
    "    df_dev[\"split\"] = 1\n",
    "    df_test[\"split\"] = 2\n",
    "    \n",
    "    # Row bind all dataframe\n",
    "    df_train_dev_test = pd.concat([df_train, df_dev, df_test])\n",
    "    \n",
    "    # Print each dataset size\n",
    "    print(\"Train size is: \" + str((1-test_ratio-dev_ratio)*100) + \"%,\", df_train.shape[0])\n",
    "    print(\"Validation size is: \" + str((dev_ratio)*100) + \"%,\" ,df_dev.shape[0])\n",
    "    print(\"Test size is: \" + str((test_ratio)*100) + \"%,\", df_test.shape[0])\n",
    "    \n",
    "    return df_train_dev_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this new add_splits() function on final_csv.csv dataset in DSI Documents folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size is: 80.0%, 11206\n",
      "Validation size is: 10.0%, 1401\n",
      "Test size is: 10.0%, 1401\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 6)\n",
    "splits_out_df = add_splits(pd.read_csv(cleaned_data_filename), 1, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcript_filepath</th>\n",
       "      <th>wave_filename</th>\n",
       "      <th>speech</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>end_timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>transcriber_id</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8115</th>\n",
       "      <td>252-3</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/252...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>character.</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:01:59.07</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11397</th>\n",
       "      <td>129-1</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/129...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>name.</td>\n",
       "      <td>00:04:59.21</td>\n",
       "      <td>00:06:00.05</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>027-3</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/027...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>and it has 2001.</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:02:36.21</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8115</th>\n",
       "      <td>252-3</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/252...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>character.</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:01:59.07</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>134-1</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/134...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>okay.</td>\n",
       "      <td>00:08:00.21</td>\n",
       "      <td>00:10:01.01</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>135-2</td>\n",
       "      <td>~ise/data/cleaned_data/cleaned_transcripts/135...</td>\n",
       "      <td>/data/p_dsi/wise/data/Audio Files &amp; Tanscripts...</td>\n",
       "      <td>they did ten times 100000.</td>\n",
       "      <td>00:04:01.23</td>\n",
       "      <td>00:06:11.27</td>\n",
       "      <td>NEU</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                transcript_filepath  \\\n",
       "8115   252-3  ~ise/data/cleaned_data/cleaned_transcripts/252...   \n",
       "11397  129-1  ~ise/data/cleaned_data/cleaned_transcripts/129...   \n",
       "4143   027-3  ~ise/data/cleaned_data/cleaned_transcripts/027...   \n",
       "...      ...                                                ...   \n",
       "8115   252-3  ~ise/data/cleaned_data/cleaned_transcripts/252...   \n",
       "1015   134-1  ~ise/data/cleaned_data/cleaned_transcripts/134...   \n",
       "761    135-2  ~ise/data/cleaned_data/cleaned_transcripts/135...   \n",
       "\n",
       "                                           wave_filename  \\\n",
       "8115   /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "11397  /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "4143   /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "...                                                  ...   \n",
       "8115   /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "1015   /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "761    /data/p_dsi/wise/data/Audio Files & Tanscripts...   \n",
       "\n",
       "                           speech start_timestamp end_timestamp label  \\\n",
       "8115                   character.     00:00:00.00   00:01:59.07   NEU   \n",
       "11397                       name.     00:04:59.21   00:06:00.05   NEU   \n",
       "4143             and it has 2001.     00:00:00.00   00:02:36.21   NEU   \n",
       "...                           ...             ...           ...   ...   \n",
       "8115                   character.     00:00:00.00   00:01:59.07   NEU   \n",
       "1015                        okay.     00:08:00.21   00:10:01.01   NEU   \n",
       "761    they did ten times 100000.     00:04:01.23   00:06:11.27   NEU   \n",
       "\n",
       "       transcriber_id  split  \n",
       "8115              198      2  \n",
       "11397             198      0  \n",
       "4143              198      0  \n",
       "...               ...    ...  \n",
       "8115              198      0  \n",
       "1015              198      0  \n",
       "761               198      0  \n",
       "\n",
       "[10 rows x 9 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_out_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write this to file just for save keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits_out_df.to_csv(base_prefix + 'cleaned_data/final_csv_wsplits.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
